{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "> Before diving into the analysis, it's essential to clean and preprocess the data. In this section, we will:\n",
    "\n",
    "1. **Load the data** from the source file.\n",
    "\n",
    "2. **Inspect** the data for missing or incorrect values.\n",
    "\n",
    "3. **Clean** the data by handling missing or incorrect values, and standardize formats.\n",
    "\n",
    "4. **Transform** the data, if necessary, to create new features or better representations of existing features for our analysis.\n",
    "\n",
    "> By carefully preparing the data, we can ensure that our analysis is accurate and reliable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Fields for Time-based Analysis to Reduce Critical Traffic Collisions\n",
    "\n",
    "The following fields have been selected for their relevance in conducting a time-based analysis with the goal of reducing critical traffic collisions by 10%. Understanding the trends and patterns in these fields can help inform data-driven policies and initiatives aimed at reducing traffic fatalities and injuries.\n",
    "\n",
    "| Column Name                   | Description                                                | Type        |\n",
    "|-------------------------------|------------------------------------------------------------|-------------|\n",
    "| CRASH DATE                    | Occurrence date of collision                               | Date & Time |\n",
    "| CRASH TIME                    | Occurrence time of collision                               | Plain Text  |\n",
    "| BOROUGH                       | Borough where collision occurred                           | Plain Text  |\n",
    "| LATITUDE                      | Latitude coordinate for Global Coordinate System           | Number      |\n",
    "| LONGITUDE                     | Longitude coordinate for Global Coordinate System          | Number      |\n",
    "| NUMBER OF PERSONS INJURED     | Number of persons injured                                  | Number      |\n",
    "| NUMBER OF PERSONS KILLED      | Number of persons killed                                   | Number      |\n",
    "| NUMBER OF PEDESTRIANS INJURED | Number of pedestrians injured                              | Number      |\n",
    "| NUMBER OF PEDESTRIANS KILLED  | Number of pedestrians killed                               | Number      |\n",
    "| NUMBER OF CYCLIST INJURED     | Number of cyclists injured                                 | Number      |\n",
    "| NUMBER OF CYCLIST KILLED      | Number of cyclists killed                                  | Number      |\n",
    "| NUMBER OF MOTORIST INJURED    | Number of vehicle occupants injured                        | Number      |\n",
    "| NUMBER OF MOTORIST KILLED     | Number of vehicle occupants killed                         | Number      |\n",
    "| CONTRIBUTING FACTOR VEHICLE 1 | Factors contributing to the collision for designated vehicle| Plain Text  |\n",
    "| CONTRIBUTING FACTOR VEHICLE 2 | Factors contributing to the collision for designated vehicle| Plain Text  |\n",
    "| VEHICLE TYPE CODE 1           | Type of vehicle involved                                   | Plain Text  |\n",
    "| VEHICLE TYPE CODE 2           | Type of vehicle involved                                   | Plain Text  |\n",
    "\n",
    "> **CRASH DATE and CRASH TIME**: \n",
    "\n",
    "Analyzing the date and time of collisions can reveal patterns, such as specific days of the week or times of the day with higher collision rates. This information can help target interventions and allocate resources effectively.\n",
    "> **BOROUGH, LATITUDE, and LONGITUDE**: \n",
    "\n",
    "Geographical data allows for the identification of high-risk areas and hotspots, as well as the evaluation of local policies and infrastructure.\n",
    "> **NUMBER OF PERSONS INJURED, NUMBER OF PERSONS KILLED, NUMBER OF PEDESTRIANS INJURED, NUMBER OF PEDESTRIANS KILLED, NUMBER OF CYCLIST INJURED, NUMBER OF CYCLIST KILLED, NUMBER OF MOTORIST INJURED, NUMBER OF MOTORIST KILLED**: \n",
    "\n",
    "These fields provide crucial information on the severity of collisions and their impact on different road users. This data can be used to prioritize interventions and track progress towards reducing critical traffic collisions.\n",
    "> **CONTRIBUTING FACTOR VEHICLE 1 and CONTRIBUTING FACTOR VEHICLE 2**: \n",
    "\n",
    "Understanding the factors contributing to collisions is essential for identifying targeted interventions and addressing the root causes of traffic collisions.\n",
    "> **VEHICLE TYPE CODE 1 and VEHICLE TYPE CODE 2**: \n",
    "\n",
    "Vehicle types can play a significant role in the severity and frequency of collisions. Analyzing vehicle type data can help identify specific vehicle types that may be overrepresented in critical traffic collisions and inform interventions tailored to those types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock One))\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_4168\\4038285749.py:4: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  crash_df = pd.read_csv(\"/Users/Administrator/Documents/Motor_Vehicle_Collisions_-_Crashes.csv\")\n"
     ]
    }
   ],
   "source": [
    "# ((Codeblock Two))\n",
    "\n",
    "#Load the NYPD Motor Collisions file\n",
    "crash_df = pd.read_csv(\"/Users/Administrator/Documents/Motor_Vehicle_Collisions_-_Crashes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1979921 entries, 0 to 1979920\n",
      "Data columns (total 29 columns):\n",
      " #   Column                         Non-Null Count    Dtype  \n",
      "---  ------                         --------------    -----  \n",
      " 0   CRASH DATE                     1979921 non-null  object \n",
      " 1   CRASH TIME                     1979921 non-null  object \n",
      " 2   BOROUGH                        1364306 non-null  object \n",
      " 3   ZIP CODE                       1364068 non-null  object \n",
      " 4   LATITUDE                       1753290 non-null  float64\n",
      " 5   LONGITUDE                      1753290 non-null  float64\n",
      " 6   LOCATION                       1753290 non-null  object \n",
      " 7   ON STREET NAME                 1566009 non-null  object \n",
      " 8   CROSS STREET NAME              1244417 non-null  object \n",
      " 9   OFF STREET NAME                321541 non-null   object \n",
      " 10  NUMBER OF PERSONS INJURED      1979903 non-null  float64\n",
      " 11  NUMBER OF PERSONS KILLED       1979890 non-null  float64\n",
      " 12  NUMBER OF PEDESTRIANS INJURED  1979921 non-null  int64  \n",
      " 13  NUMBER OF PEDESTRIANS KILLED   1979921 non-null  int64  \n",
      " 14  NUMBER OF CYCLIST INJURED      1979921 non-null  int64  \n",
      " 15  NUMBER OF CYCLIST KILLED       1979921 non-null  int64  \n",
      " 16  NUMBER OF MOTORIST INJURED     1979921 non-null  int64  \n",
      " 17  NUMBER OF MOTORIST KILLED      1979921 non-null  int64  \n",
      " 18  CONTRIBUTING FACTOR VEHICLE 1  1973784 non-null  object \n",
      " 19  CONTRIBUTING FACTOR VEHICLE 2  1680330 non-null  object \n",
      " 20  CONTRIBUTING FACTOR VEHICLE 3  139513 non-null   object \n",
      " 21  CONTRIBUTING FACTOR VEHICLE 4  31177 non-null    object \n",
      " 22  CONTRIBUTING FACTOR VEHICLE 5  8390 non-null     object \n",
      " 23  COLLISION_ID                   1979921 non-null  int64  \n",
      " 24  VEHICLE TYPE CODE 1            1967779 non-null  object \n",
      " 25  VEHICLE TYPE CODE 2            1615175 non-null  object \n",
      " 26  VEHICLE TYPE CODE 3            134775 non-null   object \n",
      " 27  VEHICLE TYPE CODE 4            30151 non-null    object \n",
      " 28  VEHICLE TYPE CODE 5            8142 non-null     object \n",
      "dtypes: float64(4), int64(7), object(18)\n",
      "memory usage: 438.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# ((Codeblock Three))\n",
    "\n",
    "crash_df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyst Notes\n",
    "\n",
    "**Total records:** 1.9 million\n",
    "\n",
    "**Records with Borough and Zip Code info:** 1.3 million\n",
    "\n",
    "This means we're missing location records (Borough and Zip Code) for approximately 600k+ entries, or ~30% of the data.\n",
    "\n",
    "> **Note:** We could potentially use reverse geocoding with the `GeoPY` library to find the address (Borough and Zip Code) for records that have a location (Latitude and Longitude). However, this process is time-consuming and may not be feasible for this exercise. As a result, we won't be conducting reverse geocoding in our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyst Notes\n",
    "## Renaming Fields for Consistency\n",
    "To ensure that our dataset aligns with our internal data model and follows a common structure, we will rename certain fields. This will make it easier to integrate our analysis with other data sources and facilitate collaboration across teams.\n",
    "\n",
    ">Follow these steps to rename fields:\n",
    "\n",
    "Review the current field names and compare them with the internal data model's requirements.\n",
    "Identify any discrepancies or inconsistencies in field names.\n",
    "Rename the fields to match the internal data model, ensuring that they adhere to a common naming convention.\n",
    "By standardizing field names, we can improve data quality and make our analysis more efficient and reliable.\n",
    "\n",
    "> **Note:** Remember to document any changes made to field names, as this will help maintain transparency and facilitate future data management tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Four))\n",
    "\n",
    "crash_df.rename(columns = {  \n",
    "\t\t'CRASH DATE' : 'crash_date',\n",
    "\t\t'CRASH TIME' : 'crash_time',\n",
    "\t\t'BOROUGH' : 'borough',\n",
    "\t\t'ZIP CODE' : 'zip_code',\n",
    "\t\t'LATITUDE' : 'latitude',\n",
    "\t\t'LONGITUDE' : 'longitude',\n",
    "\t\t'LOCATION' : 'location',\n",
    "\t\t'ON STREET NAME'    : 'street_on',\n",
    "\t\t'CROSS STREET NAME' : 'street_cross',\n",
    "\t\t'OFF STREET NAME'   : 'street_off',\n",
    "\t\t'NUMBER OF PERSONS INJURED'     : 'total_injured',\n",
    "\t\t'NUMBER OF PERSONS KILLED'      : 'total_fatality',\n",
    "\t\t'NUMBER OF PEDESTRIANS INJURED' : 'ped_injured',\n",
    "\t\t'NUMBER OF PEDESTRIANS KILLED'  : 'ped_fatality',\n",
    "\t\t'NUMBER OF CYCLIST INJURED'     : 'cyc_injured',\n",
    "\t\t'NUMBER OF CYCLIST KILLED'      : 'cyc_fatality',\n",
    "\t\t'NUMBER OF MOTORIST INJURED'    : 'moto_injured',\n",
    "\t\t'NUMBER OF MOTORIST KILLED'     : 'moto_fatality',\n",
    "\t\t'CONTRIBUTING FACTOR VEHICLE 1' : 'veh_factor_1',\n",
    "\t\t'CONTRIBUTING FACTOR VEHICLE 2' : 'veh_factor_2',\n",
    "\t\t'CONTRIBUTING FACTOR VEHICLE 3' : 'veh_factor_3',\n",
    "\t\t'CONTRIBUTING FACTOR VEHICLE 4' : 'veh_factor_4',\n",
    "\t\t'CONTRIBUTING FACTOR VEHICLE 5' : 'veh_factor_5',\n",
    "\t\t'COLLISION_ID' : 'collision_id',\n",
    "\t\t'VEHICLE TYPE CODE 1' : 'veh_type_1',\n",
    "\t\t'VEHICLE TYPE CODE 2' : 'veh_type_2',\n",
    "\t\t'VEHICLE TYPE CODE 3' : 'veh_type_3',\n",
    "\t\t'VEHICLE TYPE CODE 4' : 'veh_type_4',\n",
    "\t\t'VEHICLE TYPE CODE 5' : 'veh_type_5'\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crash_date              0\n",
       "crash_time              0\n",
       "borough            615615\n",
       "zip_code           615853\n",
       "latitude           226631\n",
       "longitude          226631\n",
       "location           226631\n",
       "street_on          413912\n",
       "street_cross       735504\n",
       "street_off        1658380\n",
       "total_injured          18\n",
       "total_fatality         31\n",
       "ped_injured             0\n",
       "ped_fatality            0\n",
       "cyc_injured             0\n",
       "cyc_fatality            0\n",
       "moto_injured            0\n",
       "moto_fatality           0\n",
       "veh_factor_1         6137\n",
       "veh_factor_2       299591\n",
       "veh_factor_3      1840408\n",
       "veh_factor_4      1948744\n",
       "veh_factor_5      1971531\n",
       "collision_id            0\n",
       "veh_type_1          12142\n",
       "veh_type_2         364746\n",
       "veh_type_3        1845146\n",
       "veh_type_4        1949770\n",
       "veh_type_5        1971779\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ((Codeblock Five))\n",
    "\n",
    "# Find all the keys with missing values and validate our data model changes\n",
    "crash_df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyst Notes\n",
    "\n",
    "> Assign missing Borough records to the value of 'NYC'\n",
    "\n",
    "Borough and Zip Code are missing for ~600k records, which is ~30% of the data. This is a significant portion, so we can't disregard it. We'll have 5 boroughs plus 'NYC' to tag the data with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crash_date              0\n",
       "crash_time              0\n",
       "borough                 0\n",
       "zip_code           615853\n",
       "latitude           226631\n",
       "longitude          226631\n",
       "location           226631\n",
       "street_on          413912\n",
       "street_cross       735504\n",
       "street_off        1658380\n",
       "total_injured          18\n",
       "total_fatality         31\n",
       "ped_injured             0\n",
       "ped_fatality            0\n",
       "cyc_injured             0\n",
       "cyc_fatality            0\n",
       "moto_injured            0\n",
       "moto_fatality           0\n",
       "veh_factor_1         6137\n",
       "veh_factor_2       299591\n",
       "veh_factor_3      1840408\n",
       "veh_factor_4      1948744\n",
       "veh_factor_5      1971531\n",
       "collision_id            0\n",
       "veh_type_1          12142\n",
       "veh_type_2         364746\n",
       "veh_type_3        1845146\n",
       "veh_type_4        1949770\n",
       "veh_type_5        1971779\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ((Codeblock Six))\n",
    "\n",
    "# Fill all blank values in column Borough\n",
    "# If a value is NaN it will be NYC\n",
    "crash_df.loc[crash_df['borough'].isnull(), 'borough'] = 'NYC'\n",
    "\n",
    "# View the output;`borough` should have 0 NaN values\n",
    "crash_df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyst Notes\n",
    " \n",
    "> Filter Out Total Injured and Total Fatality NaN values\n",
    "\n",
    "We should only keep records where the total number of injured and killed is greater than 0. This will ensure that we're focusing on relevant incidents in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Seven))\n",
    "\n",
    "# Remove NaN from TOTAL INJURED\n",
    "crash_df = crash_df.dropna(axis=0, subset=['total_injured'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Eight))\n",
    "\n",
    "# Remove NaN from TOTAL KILLED\n",
    "crash_df = crash_df.dropna(axis=0, subset=['total_fatality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Nine))\n",
    "\n",
    "# If we are interested in collisions that have injuries, then we may  want to keep those values > 0 as df1\n",
    "df1 = crash_df[(crash_df['total_injured'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Ten))\n",
    "\n",
    "# If we are interested in the collisions that have fatalities, then we may want to keep those values > 0 as df2\n",
    "df2 = crash_df[(crash_df['total_fatality'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Eleven))\n",
    "\n",
    "# To keep only those records with either injuries or fatalities we can  now concatenate df1 and df2 and put it back as df\n",
    "crash_df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Twelve))\n",
    "\n",
    "# Combine DATE and TIME column to transform Series to DateTime needed for further analysis\n",
    "crash_df['date'] = crash_df['crash_date'] + ' ' + crash_df['crash_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Thirteen))\n",
    "\n",
    "# Convert string to DateTime\n",
    "crash_df['date'] = pd.to_datetime(crash_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Fourteen))\n",
    "\n",
    "# Year filter\n",
    "crash_df['year'] = pd.to_datetime(crash_df['date']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Fifteen))\n",
    "\n",
    "# Quarter filter\n",
    "crash_df['quarter'] = pd.to_datetime(crash_df['date']).dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Sixteen))\n",
    "\n",
    "# Month filter\n",
    "crash_df['month'] = pd.to_datetime(crash_df['date']).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Seventeen))\n",
    "\n",
    "# Day of the week filter\n",
    "crash_df['weekday'] = pd.to_datetime(crash_df['date']).dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Eighteen))\n",
    "\n",
    "#Fill in missing values with Empty\n",
    "crash_df = crash_df.fillna(value='EMPTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 436524 entries, 0 to 1979649\n",
      "Data columns (total 34 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   crash_date      436524 non-null  object        \n",
      " 1   crash_time      436524 non-null  object        \n",
      " 2   borough         436524 non-null  object        \n",
      " 3   zip_code        436524 non-null  object        \n",
      " 4   latitude        436524 non-null  object        \n",
      " 5   longitude       436524 non-null  object        \n",
      " 6   location        436524 non-null  object        \n",
      " 7   street_on       436524 non-null  object        \n",
      " 8   street_cross    436524 non-null  object        \n",
      " 9   street_off      436524 non-null  object        \n",
      " 10  total_injured   436524 non-null  float64       \n",
      " 11  total_fatality  436524 non-null  float64       \n",
      " 12  ped_injured     436524 non-null  int64         \n",
      " 13  ped_fatality    436524 non-null  int64         \n",
      " 14  cyc_injured     436524 non-null  int64         \n",
      " 15  cyc_fatality    436524 non-null  int64         \n",
      " 16  moto_injured    436524 non-null  int64         \n",
      " 17  moto_fatality   436524 non-null  int64         \n",
      " 18  veh_factor_1    436524 non-null  object        \n",
      " 19  veh_factor_2    436524 non-null  object        \n",
      " 20  veh_factor_3    436524 non-null  object        \n",
      " 21  veh_factor_4    436524 non-null  object        \n",
      " 22  veh_factor_5    436524 non-null  object        \n",
      " 23  collision_id    436524 non-null  int64         \n",
      " 24  veh_type_1      436524 non-null  object        \n",
      " 25  veh_type_2      436524 non-null  object        \n",
      " 26  veh_type_3      436524 non-null  object        \n",
      " 27  veh_type_4      436524 non-null  object        \n",
      " 28  veh_type_5      436524 non-null  object        \n",
      " 29  date            436524 non-null  datetime64[ns]\n",
      " 30  year            436524 non-null  int64         \n",
      " 31  quarter         436524 non-null  int64         \n",
      " 32  month           436524 non-null  int64         \n",
      " 33  weekday         436524 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(2), int64(11), object(20)\n",
      "memory usage: 116.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# ((Codeblock Nineteenth))\n",
    "\n",
    "# Validate the final dataset before analysis\n",
    "crash_df.info(verbose = True, show_counts = True)\n",
    "# We have ~430k relevant records instead of 1.9 million and our file is ~100 MB from ~400 MB at the beginning of preparation\n",
    "# This file is in a much better state to ingest into LogScale and begin working with a sample of the data.\n",
    "# Additionally, we have are steps to clean the data within LogScale via parser as ingest is streaming via API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Twenty))\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV File\n",
    "crash_df.to_csv(\"/Users/Administrator/Documents/clean_nyc_crash.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
